{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f596f0f6",
   "metadata": {},
   "source": [
    "# Word2Vec Analysis for News Articles\n",
    "\n",
    "This notebook implements Word2Vec embeddings for analyzing news articles from Articles.csv dataset.\n",
    "\n",
    "## Features:\n",
    "- Text preprocessing and cleaning\n",
    "- Word2Vec model training\n",
    "- Document similarity analysis\n",
    "- Clustering and visualization\n",
    "- Temporal analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b89c2",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "093f91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install gensim nltk scikit-learn matplotlib seaborn wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# NLP libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Word2Vec and ML libraries\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('max_colwidth', 100)\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úì Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6869d9",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1887132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the articles dataset\n",
    "df = pd.read_csv('Articles.csv')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e8f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data structure\n",
    "print(\"Sample article text:\")\n",
    "print(df['Article text'].iloc[0][:500] + \"...\")\n",
    "\n",
    "print(f\"\\nAverage article length: {df['Article text'].str.len().mean():.0f} characters\")\n",
    "print(f\"Median article length: {df['Article text'].str.len().median():.0f} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ead4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categories and sections\n",
    "if 'Category' in df.columns:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    df['Category'].value_counts().plot(kind='bar')\n",
    "    plt.title('Articles by Category')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    df['Section'].value_counts().head(10).plot(kind='bar')\n",
    "    plt.title('Top 10 Sections')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3137c963",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3ca9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return []\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove special characters, keep only letters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # Apply stemming\n",
    "    words = [ps.stem(word) for word in words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Test preprocessing\n",
    "sample_text = df['Article text'].iloc[0][:200]\n",
    "processed_sample = preprocess_text(sample_text)\n",
    "\n",
    "print(\"Original text (first 200 chars):\")\n",
    "print(sample_text)\n",
    "print(\"\\nProcessed tokens (first 15):\")\n",
    "print(processed_sample[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc90aaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all articles\n",
    "print(\"Preprocessing all articles...\")\n",
    "df['processed_tokens'] = df['Article text'].apply(preprocess_text)\n",
    "\n",
    "# Remove empty documents\n",
    "df = df[df['processed_tokens'].apply(len) > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"Number of articles after preprocessing: {len(df)}\")\n",
    "print(f\"Average tokens per article: {df['processed_tokens'].apply(len).mean():.1f}\")\n",
    "print(f\"Median tokens per article: {df['processed_tokens'].apply(len).median():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93005bef",
   "metadata": {},
   "source": [
    "## 4. Word2Vec Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780fe972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare sentences for Word2Vec\n",
    "sentences = df['processed_tokens'].tolist()\n",
    "\n",
    "# Train Word2Vec model\n",
    "print(\"Training Word2Vec model...\")\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,      # Dimensionality of word vectors\n",
    "    window=5,             # Context window size  \n",
    "    min_count=3,          # Ignore words with frequency less than this\n",
    "    workers=4,            # Number of worker threads\n",
    "    epochs=10,            # Number of training epochs\n",
    "    sg=0                  # 0 for CBOW, 1 for Skip-gram\n",
    ")\n",
    "\n",
    "print(f\"‚úì Word2Vec model trained successfully!\")\n",
    "print(f\"‚úì Vocabulary size: {len(w2v_model.wv.key_to_index)}\")\n",
    "print(f\"‚úì Vector dimensionality: {w2v_model.wv.vector_size}\")\n",
    "\n",
    "# Save the model\n",
    "w2v_model.save('articles_word2vec.model')\n",
    "print(\"‚úì Model saved as 'articles_word2vec.model'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905c40c",
   "metadata": {},
   "source": [
    "## 5. Word Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce37798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common words\n",
    "all_words = []\n",
    "for tokens in sentences:\n",
    "    all_words.extend(tokens)\n",
    "\n",
    "word_freq = pd.Series(all_words).value_counts()\n",
    "print(\"Top 20 most frequent words:\")\n",
    "print(word_freq.head(20))\n",
    "\n",
    "# Visualize word frequencies\n",
    "plt.figure(figsize=(12, 6))\n",
    "word_freq.head(20).plot(kind='bar')\n",
    "plt.title('Top 20 Most Frequent Words')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fcfd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test word similarities\n",
    "test_words = ['econom', 'technolog', 'russia', 'china', 'covid', 'climat', 'energi']\n",
    "available_words = [word for word in test_words if word in w2v_model.wv.key_to_index]\n",
    "\n",
    "print(f\"Testing similarities for available words: {available_words}\")\n",
    "\n",
    "for word in available_words[:5]:  # Test first 5 available words\n",
    "    try:\n",
    "        similar_words = w2v_model.wv.most_similar(word, topn=5)\n",
    "        print(f\"\\nüîç Words similar to '{word}':\")\n",
    "        for sim_word, similarity in similar_words:\n",
    "            print(f\"   {sim_word}: {similarity:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding similarities for '{word}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eccd08",
   "metadata": {},
   "source": [
    "## 6. Document Vector Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949a5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_vector(tokens, model, vector_size=100):\n",
    "    \"\"\"Get document vector by averaging word vectors\"\"\"\n",
    "    vectors = []\n",
    "    for token in tokens:\n",
    "        if token in model.wv.key_to_index:\n",
    "            vectors.append(model.wv[token])\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "\n",
    "# Create document vectors\n",
    "print(\"Creating document vectors...\")\n",
    "doc_vectors = []\n",
    "for tokens in df['processed_tokens']:\n",
    "    vector = get_document_vector(tokens, w2v_model)\n",
    "    doc_vectors.append(vector)\n",
    "\n",
    "doc_vectors = np.array(doc_vectors)\n",
    "print(f\"‚úì Document vectors created: {doc_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f0d620",
   "metadata": {},
   "source": [
    "## 7. Document Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering\n",
    "n_clusters = 6\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "clusters = kmeans.fit_predict(doc_vectors)\n",
    "\n",
    "# Add cluster labels to dataframe\n",
    "df['cluster'] = clusters\n",
    "\n",
    "print(f\"Documents clustered into {n_clusters} groups:\")\n",
    "cluster_counts = pd.Series(clusters).value_counts().sort_index()\n",
    "print(cluster_counts)\n",
    "\n",
    "# Visualize cluster distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "cluster_counts.plot(kind='bar')\n",
    "plt.title('Document Distribution Across Clusters')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2021d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample headlines from each cluster\n",
    "print(\"üì∞ Sample headlines from each cluster:\")\n",
    "for i in range(n_clusters):\n",
    "    cluster_docs = df[df['cluster'] == i]\n",
    "    if len(cluster_docs) > 0:\n",
    "        print(f\"\\nüè∑Ô∏è Cluster {i} ({len(cluster_docs)} articles):\")\n",
    "        for j, headline in enumerate(cluster_docs['Headline'].head(3)):\n",
    "            print(f\"   {j+1}. {headline[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13d91a",
   "metadata": {},
   "source": [
    "## 8. Visualization with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411d8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction with PCA\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "doc_vectors_2d = pca.fit_transform(doc_vectors)\n",
    "\n",
    "# Create main visualization\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Main scatter plot\n",
    "plt.subplot(2, 2, 1)\n",
    "scatter = plt.scatter(doc_vectors_2d[:, 0], doc_vectors_2d[:, 1], \n",
    "                     c=clusters, cmap='tab10', alpha=0.7, s=50)\n",
    "plt.colorbar(scatter)\n",
    "plt.title('Document Clusters (Word2Vec + PCA)')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Color by category if available\n",
    "if 'Category' in df.columns:\n",
    "    plt.subplot(2, 2, 2)\n",
    "    categories = df['Category'].astype('category')\n",
    "    scatter = plt.scatter(doc_vectors_2d[:, 0], doc_vectors_2d[:, 1], \n",
    "                         c=categories.cat.codes, cmap='Set3', alpha=0.7, s=50)\n",
    "    plt.title('Documents by Category')\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%} variance)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Explained variance\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(['PC1', 'PC2'], pca.explained_variance_ratio_)\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "\n",
    "# Cluster sizes\n",
    "plt.subplot(2, 2, 4)\n",
    "cluster_counts.plot(kind='pie', autopct='%1.1f%%')\n",
    "plt.title('Cluster Distribution')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('word2vec_analysis_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f08080",
   "metadata": {},
   "source": [
    "## 9. Document Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1262dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_documents(doc_index, doc_vectors, df, top_n=3):\n",
    "    \"\"\"Find most similar documents to a given document\"\"\"\n",
    "    target_vector = doc_vectors[doc_index].reshape(1, -1)\n",
    "    similarities = cosine_similarity(target_vector, doc_vectors)[0]\n",
    "    \n",
    "    # Get indices of most similar documents (excluding the document itself)\n",
    "    similar_indices = np.argsort(similarities)[::-1][1:top_n+1]\n",
    "    \n",
    "    return similar_indices, similarities[similar_indices]\n",
    "\n",
    "# Test similarity for first few documents\n",
    "print(\"üîç Document Similarity Analysis\")\n",
    "\n",
    "for test_idx in [0, 10, 20]:  # Test multiple documents\n",
    "    if test_idx < len(df):\n",
    "        print(f\"\\nüìÑ Document {test_idx}:\")\n",
    "        print(f\"Headline: {df.iloc[test_idx]['Headline'][:80]}...\")\n",
    "        \n",
    "        similar_indices, similarities = find_similar_documents(test_idx, doc_vectors, df, top_n=3)\n",
    "        \n",
    "        print(\"Most similar articles:\")\n",
    "        for i, (idx, sim) in enumerate(zip(similar_indices, similarities)):\n",
    "            headline = df.iloc[idx]['Headline']\n",
    "            print(f\"  {i+1}. Similarity: {sim:.3f}\")\n",
    "            print(f\"     {headline[:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90078d7a",
   "metadata": {},
   "source": [
    "## 10. Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ae2358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column if available\n",
    "if 'Date published' in df.columns:\n",
    "    df['date'] = pd.to_datetime(df['Date published'], errors='coerce')\n",
    "    \n",
    "    # Group by month\n",
    "    monthly_counts = df.groupby(df['date'].dt.to_period('M')).size()\n",
    "    \n",
    "    print(\"üìÖ Articles per month:\")\n",
    "    print(monthly_counts.tail(12))  # Show last 12 months\n",
    "    \n",
    "    # Plot temporal trends\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Overall trend\n",
    "    plt.subplot(2, 2, 1)\n",
    "    monthly_counts.plot(kind='line', marker='o')\n",
    "    plt.title('Articles Over Time')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Category trends if available\n",
    "    if 'Category' in df.columns:\n",
    "        plt.subplot(2, 2, 2)\n",
    "        category_time = df.groupby([df['date'].dt.to_period('M'), 'Category']).size().unstack(fill_value=0)\n",
    "        category_time.plot(kind='area', stacked=True)\n",
    "        plt.title('Categories Over Time')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Number of Articles')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    # Cluster trends\n",
    "    plt.subplot(2, 2, 3)\n",
    "    cluster_time = df.groupby([df['date'].dt.to_period('M'), 'cluster']).size().unstack(fill_value=0)\n",
    "    cluster_time.plot(kind='line', marker='o')\n",
    "    plt.title('Clusters Over Time')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Articles')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Day of week pattern\n",
    "    plt.subplot(2, 2, 4)\n",
    "    day_pattern = df['date'].dt.day_name().value_counts()\n",
    "    day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    day_pattern = day_pattern.reindex(day_order)\n",
    "    day_pattern.plot(kind='bar')\n",
    "    plt.title('Articles by Day of Week')\n",
    "    plt.xlabel('Day')\n",
    "    plt.ylabel('Total Articles')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('temporal_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b911b57c",
   "metadata": {},
   "source": [
    "## 11. Advanced Analysis: Topic Keywords per Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6573e4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dominant words in each cluster\n",
    "print(\"üè∑Ô∏è Dominant words in each cluster:\")\n",
    "\n",
    "for cluster_id in range(n_clusters):\n",
    "    cluster_docs = df[df['cluster'] == cluster_id]\n",
    "    \n",
    "    # Get all tokens from this cluster\n",
    "    cluster_tokens = []\n",
    "    for tokens in cluster_docs['processed_tokens']:\n",
    "        cluster_tokens.extend(tokens)\n",
    "    \n",
    "    # Count word frequencies\n",
    "    token_freq = pd.Series(cluster_tokens).value_counts()\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id} ({len(cluster_docs)} articles):\")\n",
    "    print(f\"Top keywords: {', '.join(token_freq.head(10).index)}\")\n",
    "    \n",
    "    # Show category distribution for this cluster\n",
    "    if 'Category' in df.columns:\n",
    "        cluster_categories = cluster_docs['Category'].value_counts()\n",
    "        print(f\"Main categories: {dict(cluster_categories.head(3))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ff0c2",
   "metadata": {},
   "source": [
    "## 12. Model Evaluation and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd72fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary and statistics\n",
    "print(\"üìä WORD2VEC MODEL SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"‚úì Total articles processed: {len(df):,}\")\n",
    "print(f\"‚úì Vocabulary size: {len(w2v_model.wv.key_to_index):,} unique words\")\n",
    "print(f\"‚úì Vector dimensionality: {w2v_model.wv.vector_size}\")\n",
    "print(f\"‚úì Average tokens per article: {df['processed_tokens'].apply(len).mean():.1f}\")\n",
    "print(f\"‚úì Documents clustered into: {n_clusters} groups\")\n",
    "print(f\"‚úì PCA explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "\n",
    "if 'Category' in df.columns:\n",
    "    print(f\"‚úì Categories covered: {df['Category'].nunique()}\")\n",
    "    \n",
    "if 'Date published' in df.columns:\n",
    "    date_range = df['date'].max() - df['date'].min()\n",
    "    print(f\"‚úì Time span: {date_range.days} days\")\n",
    "\n",
    "print(\"\\nüéØ KEY INSIGHTS:\")\n",
    "print(f\"‚Ä¢ Most frequent words: {', '.join(word_freq.head(5).index)}\")\n",
    "print(f\"‚Ä¢ Largest cluster: {cluster_counts.max()} articles (Cluster {cluster_counts.idxmax()})\")\n",
    "print(f\"‚Ä¢ Smallest cluster: {cluster_counts.min()} articles (Cluster {cluster_counts.idxmin()})\")\n",
    "\n",
    "print(\"\\nüíæ SAVED FILES:\")\n",
    "print(\"‚Ä¢ articles_word2vec.model - Trained Word2Vec model\")\n",
    "print(\"‚Ä¢ word2vec_analysis_dashboard.png - Main visualization\")\n",
    "print(\"‚Ä¢ temporal_analysis.png - Time series analysis\")\n",
    "\n",
    "print(\"\\nüöÄ NEXT STEPS:\")\n",
    "print(\"‚Ä¢ Experiment with different vector sizes and window sizes\")\n",
    "print(\"‚Ä¢ Try Skip-gram vs CBOW models\")\n",
    "print(\"‚Ä¢ Compare with TF-IDF or other embeddings\")\n",
    "print(\"‚Ä¢ Build classification models using these embeddings\")\n",
    "print(\"‚Ä¢ Explore topic modeling with LDA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7a69f3",
   "metadata": {},
   "source": [
    "## 13. Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045df140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples of how to use the trained model\n",
    "print(\"üìñ USAGE EXAMPLES:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# 1. Load saved model\n",
    "print(\"\\n1. Load the saved model:\")\n",
    "print(\"   from gensim.models import Word2Vec\")\n",
    "print(\"   model = Word2Vec.load('articles_word2vec.model')\")\n",
    "\n",
    "# 2. Get word vector\n",
    "if 'econom' in w2v_model.wv.key_to_index:\n",
    "    vector = w2v_model.wv['econom']\n",
    "    print(f\"\\n2. Get word vector (example for 'econom'):\")\n",
    "    print(f\"   vector = model.wv['econom']\")\n",
    "    print(f\"   Shape: {vector.shape}, First 5 values: {vector[:5]}\")\n",
    "\n",
    "# 3. Find similar words\n",
    "print(\"\\n3. Find similar words:\")\n",
    "print(\"   similar = model.wv.most_similar('word', topn=5)\")\n",
    "\n",
    "# 4. Calculate similarity\n",
    "vocab_words = list(w2v_model.wv.key_to_index.keys())\n",
    "if len(vocab_words) >= 2:\n",
    "    word1, word2 = vocab_words[0], vocab_words[1]\n",
    "    similarity = w2v_model.wv.similarity(word1, word2)\n",
    "    print(f\"\\n4. Calculate word similarity:\")\n",
    "    print(f\"   similarity = model.wv.similarity('{word1}', '{word2}')\")\n",
    "    print(f\"   Result: {similarity:.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete! Check the saved files and model for further use.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
