{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89eecc9e",
   "metadata": {},
   "source": [
    "# ChatGPT Tweets Sentiment Analysis - Complete Beginner's Guide\n",
    "\n",
    "Welcome to the comprehensive analysis of ChatGPT tweets! In this notebook, we'll work with the `file.csv` dataset containing tweets about ChatGPT and their sentiment labels.\n",
    "\n",
    "## What you'll learn:\n",
    "- üìä Data exploration and visualization\n",
    "- üßπ Text preprocessing for NLP\n",
    "- ü§ñ Multiple machine learning approaches\n",
    "- üìà Model comparison and evaluation\n",
    "- üî§ Word embeddings with Word2Vec\n",
    "\n",
    "Let's start from the very beginning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab87c183",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, let's import all the libraries we'll need for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential libraries for data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text processing libraries\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Advanced NLP (optional)\n",
    "try:\n",
    "    from gensim.models import Word2Vec\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.decomposition import PCA\n",
    "    print(\"‚úì Advanced NLP libraries loaded (Word2Vec available)\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Gensim not available. Install with: pip install gensim\")\n",
    "\n",
    "# Download NLTK data\n",
    "print(\"Downloading NLTK data...\")\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "print(\"‚úì NLTK data downloaded\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üéâ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e836a961",
   "metadata": {},
   "source": [
    "## 2. Load the CSV File\n",
    "\n",
    "Let's load our dataset and take a first look at what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484085cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('file.csv')\n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"üìä Dataset shape: {df.shape}\")\n",
    "    print(f\"üìã Columns: {df.columns.tolist()}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå file.csv not found. Please ensure the file is in the current directory.\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüîç First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c199b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what our data looks like\n",
    "print(\"üìù Sample tweets and their labels:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nTweet {i+1}:\")\n",
    "    print(f\"Text: {df['tweets'].iloc[i][:100]}...\")\n",
    "    print(f\"Label: {df['labels'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfabc7c0",
   "metadata": {},
   "source": [
    "## 3. Explore the Dataset Structure\n",
    "\n",
    "Now let's get a comprehensive understanding of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0147e2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"üìä DATASET OVERVIEW\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Number of rows: {df.shape[0]:,}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(f\"\\nColumn information:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56206691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and basic statistics\n",
    "print(\"üìà BASIC STATISTICS\")\n",
    "print(\"=\" * 50)\n",
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9045b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the sentiment labels\n",
    "print(\"üòäüòêüòû SENTIMENT ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "sentiment_counts = df['labels'].value_counts()\n",
    "sentiment_percentages = df['labels'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Sentiment distribution:\")\n",
    "for label, count in sentiment_counts.items():\n",
    "    percentage = sentiment_percentages[label]\n",
    "    print(f\"{label}: {count:,} tweets ({percentage:.2f}%)\")\n",
    "\n",
    "# Let's see if the dataset is balanced\n",
    "print(f\"\\nDataset balance:\")\n",
    "if sentiment_percentages.max() - sentiment_percentages.min() < 20:\n",
    "    print(\"‚úÖ Dataset is relatively balanced\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Dataset is imbalanced - consider this in model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f55b5b9",
   "metadata": {},
   "source": [
    "## 4. Handle Missing Data\n",
    "\n",
    "Let's check for any missing values and handle them appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c09307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"üîç MISSING DATA ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "missing_data = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_data)\n",
    "\n",
    "if missing_data.sum() > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Total missing values: {missing_data.sum()}\")\n",
    "    print(\"We'll need to handle these missing values.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ No missing values found!\")\n",
    "\n",
    "# Check for empty strings or very short tweets\n",
    "empty_tweets = df['tweets'].str.len() < 10\n",
    "print(f\"\\nTweets with less than 10 characters: {empty_tweets.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59635789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the data\n",
    "print(\"üßπ DATA CLEANING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Remove rows with missing tweets or labels\n",
    "initial_size = len(df)\n",
    "df = df.dropna(subset=['tweets', 'labels'])\n",
    "after_dropna = len(df)\n",
    "\n",
    "# Remove very short tweets (less than 10 characters)\n",
    "df = df[df['tweets'].str.len() >= 10].reset_index(drop=True)\n",
    "final_size = len(df)\n",
    "\n",
    "print(f\"Initial dataset size: {initial_size:,}\")\n",
    "print(f\"After removing missing values: {after_dropna:,}\")\n",
    "print(f\"After removing short tweets: {final_size:,}\")\n",
    "print(f\"Removed {initial_size - final_size:,} rows ({((initial_size - final_size)/initial_size)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64420908",
   "metadata": {},
   "source": [
    "## 5. Basic Data Analysis\n",
    "\n",
    "Let's analyze the characteristics of our tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02119cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length analysis\n",
    "df['tweet_length'] = df['tweets'].str.len()\n",
    "df['word_count'] = df['tweets'].str.split().str.len()\n",
    "\n",
    "print(\"üìè TEXT LENGTH ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Tweet length statistics (characters):\")\n",
    "print(df['tweet_length'].describe())\n",
    "\n",
    "print(\"\\nWord count statistics:\")\n",
    "print(df['word_count'].describe())\n",
    "\n",
    "# Analysis by sentiment\n",
    "print(\"\\nüìä ANALYSIS BY SENTIMENT\")\n",
    "print(\"=\" * 50)\n",
    "for sentiment in df['labels'].unique():\n",
    "    subset = df[df['labels'] == sentiment]\n",
    "    print(f\"\\n{sentiment.upper()} tweets:\")\n",
    "    print(f\"  Average length: {subset['tweet_length'].mean():.1f} characters\")\n",
    "    print(f\"  Average words: {subset['word_count'].mean():.1f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365791f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most common words (simple analysis)\n",
    "print(\"üî§ MOST COMMON WORDS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Combine all tweets and count words\n",
    "all_text = ' '.join(df['tweets'].str.lower())\n",
    "words = re.findall(r'\\b\\w+\\b', all_text)\n",
    "word_freq = pd.Series(words).value_counts()\n",
    "\n",
    "print(\"Top 20 most common words:\")\n",
    "for i, (word, count) in enumerate(word_freq.head(20).items(), 1):\n",
    "    print(f\"{i:2d}. {word}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f693092c",
   "metadata": {},
   "source": [
    "## 6. Data Visualization\n",
    "\n",
    "Now let's create visualizations to better understand our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c1d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Sentiment distribution (bar chart)\n",
    "sentiment_counts.plot(kind='bar', ax=axes[0,0], color=['lightgreen', 'lightcoral', 'lightblue'])\n",
    "axes[0,0].set_title('Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Sentiment')\n",
    "axes[0,0].set_ylabel('Number of Tweets')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Sentiment distribution (pie chart)\n",
    "axes[0,1].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%', \n",
    "              colors=['lightgreen', 'lightcoral', 'lightblue'])\n",
    "axes[0,1].set_title('Sentiment Distribution (%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 3. Tweet length by sentiment\n",
    "df.boxplot(column='tweet_length', by='labels', ax=axes[0,2])\n",
    "axes[0,2].set_title('Tweet Length by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[0,2].set_xlabel('Sentiment')\n",
    "axes[0,2].set_ylabel('Tweet Length (characters)')\n",
    "\n",
    "# 4. Tweet length distribution\n",
    "axes[1,0].hist(df['tweet_length'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[1,0].set_title('Distribution of Tweet Lengths', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Tweet Length (characters)')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# 5. Word count by sentiment\n",
    "df.boxplot(column='word_count', by='labels', ax=axes[1,1])\n",
    "axes[1,1].set_title('Word Count by Sentiment', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Sentiment')\n",
    "axes[1,1].set_ylabel('Word Count')\n",
    "\n",
    "# 6. Word count distribution\n",
    "axes[1,2].hist(df['word_count'], bins=50, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[1,2].set_title('Distribution of Word Counts', fontsize=14, fontweight='bold')\n",
    "axes[1,2].set_xlabel('Word Count')\n",
    "axes[1,2].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('chatgpt_tweets_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis: Sentiment by word count ranges\n",
    "print(\"üìä SENTIMENT ANALYSIS BY WORD COUNT RANGES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create word count bins\n",
    "df['word_range'] = pd.cut(df['word_count'], bins=[0, 10, 20, 30, 100], \n",
    "                         labels=['1-10', '11-20', '21-30', '30+'])\n",
    "\n",
    "# Cross-tabulation\n",
    "sentiment_by_words = pd.crosstab(df['word_range'], df['labels'], normalize='index') * 100\n",
    "\n",
    "print(\"Sentiment distribution by word count ranges (%):\")\n",
    "print(sentiment_by_words.round(2))\n",
    "\n",
    "# Visualize this\n",
    "plt.figure(figsize=(10, 6))\n",
    "sentiment_by_words.plot(kind='bar', stacked=True)\n",
    "plt.title('Sentiment Distribution by Word Count Ranges', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Word Count Range')\n",
    "plt.ylabel('Percentage')\n",
    "plt.legend(title='Sentiment')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110e8cde",
   "metadata": {},
   "source": [
    "## 7. Text Preprocessing\n",
    "\n",
    "Before we can apply machine learning, we need to clean and preprocess our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c735c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessing tools\n",
    "ps = PorterStemmer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    \"\"\"Remove URLs, mentions, hashtags, and special characters\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions (@username)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    # Remove hashtags (#hashtag)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Complete text preprocessing pipeline\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove special characters, URLs, mentions, hashtags\n",
    "    text = remove_special_characters(text)\n",
    "    \n",
    "    # Tokenize and remove stopwords\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words and len(word) > 2]\n",
    "    \n",
    "    # Apply stemming\n",
    "    words = [ps.stem(word) for word in words]\n",
    "    \n",
    "    return ' '.join(words)\n",
    "\n",
    "print(\"üßπ TEXT PREPROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Preprocessing tweets... This may take a moment.\")\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_tweets'] = df['tweets'].apply(preprocess_text)\n",
    "\n",
    "# Remove tweets that became empty after preprocessing\n",
    "df = df[df['cleaned_tweets'].str.len() > 0].reset_index(drop=True)\n",
    "\n",
    "print(f\"‚úÖ Preprocessing complete!\")\n",
    "print(f\"üìä Final dataset size: {len(df):,} tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95650c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show preprocessing examples\n",
    "print(\"üìù PREPROCESSING EXAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original:  {df['tweets'].iloc[i][:100]}...\")\n",
    "    print(f\"Cleaned:   {df['cleaned_tweets'].iloc[i][:100]}...\")\n",
    "    print(f\"Label:     {df['labels'].iloc[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba11789",
   "metadata": {},
   "source": [
    "## 8. Machine Learning Models\n",
    "\n",
    "Now let's build and compare different machine learning models for sentiment classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72457e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for machine learning\n",
    "print(\"ü§ñ MACHINE LEARNING SETUP\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "X = df['cleaned_tweets']\n",
    "y = df['labels']\n",
    "\n",
    "# Split the data (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train):,}\")\n",
    "print(f\"Test set size: {len(X_test):,}\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "print(\"\\nClass distribution in test set:\")\n",
    "print(y_test.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67c516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Bag of Words (CountVectorizer)\n",
    "print(\"üìä METHOD 1: BAG OF WORDS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create Bag of Words features\n",
    "cv = CountVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_bow = cv.fit_transform(X_train)\n",
    "X_test_bow = cv.transform(X_test)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_train_bow.shape}\")\n",
    "print(f\"Feature names example: {cv.get_feature_names_out()[:10]}\")\n",
    "\n",
    "# Train Random Forest with Bag of Words\n",
    "rf_bow = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_bow = rf_bow.predict(X_test_bow)\n",
    "accuracy_bow = accuracy_score(y_test, y_pred_bow)\n",
    "\n",
    "print(f\"\\nüéØ Bag of Words + Random Forest Results:\")\n",
    "print(f\"Accuracy: {accuracy_bow:.4f} ({accuracy_bow*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170a5a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: TF-IDF with multiple algorithms\n",
    "print(\"üìä METHOD 2: TF-IDF\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create TF-IDF features\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF feature matrix shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Define models to test\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = accuracy\n",
    "    predictions[name] = y_pred\n",
    "    \n",
    "    print(f\"‚úÖ {name} Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9771881a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"üèÜ MODEL COMPARISON\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "all_results = {\n",
    "    'Bag of Words + Random Forest': accuracy_bow,\n",
    "    **{f'TF-IDF + {name}': acc for name, acc in results.items()}\n",
    "}\n",
    "\n",
    "# Sort by accuracy\n",
    "sorted_results = sorted(all_results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Model Performance Ranking:\")\n",
    "for i, (model_name, accuracy) in enumerate(sorted_results, 1):\n",
    "    print(f\"{i}. {model_name}: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Find best model\n",
    "best_model_name = sorted_results[0][0]\n",
    "best_accuracy = sorted_results[0][1]\n",
    "\n",
    "print(f\"\\nü•á Best Model: {best_model_name}\")\n",
    "print(f\"üéØ Best Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318805f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of the best TF-IDF model\n",
    "best_tfidf_model = max(results, key=results.get)\n",
    "best_model = models[best_tfidf_model]\n",
    "best_predictions = predictions[best_tfidf_model]\n",
    "\n",
    "print(f\"üìä DETAILED ANALYSIS: {best_tfidf_model}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, best_predictions))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=best_model.classes_, \n",
    "            yticklabels=best_model.classes_)\n",
    "plt.title(f'Confusion Matrix - TF-IDF + {best_tfidf_model}', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Predicted Sentiment')\n",
    "plt.ylabel('Actual Sentiment')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae0aead",
   "metadata": {},
   "source": [
    "## 9. Word2Vec Analysis (Advanced)\n",
    "\n",
    "Let's explore Word2Vec embeddings for deeper text understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247178c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Analysis\n",
    "print(\"üî§ WORD2VEC ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "try:\n",
    "    # Prepare sentences for Word2Vec (list of word lists)\n",
    "    sentences = [text.split() for text in df['cleaned_tweets'] if text.strip()]\n",
    "    \n",
    "    print(f\"Preparing {len(sentences):,} sentences for Word2Vec training...\")\n",
    "    \n",
    "    # Train Word2Vec model\n",
    "    w2v_model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=100,      # Dimensionality of word vectors\n",
    "        window=5,             # Context window size\n",
    "        min_count=5,          # Ignore words with frequency less than this\n",
    "        workers=4,            # Number of worker threads\n",
    "        epochs=10             # Number of training epochs\n",
    "    )\n",
    "    \n",
    "    vocab_size = len(w2v_model.wv.key_to_index)\n",
    "    print(f\"‚úÖ Word2Vec model trained successfully!\")\n",
    "    print(f\"üìö Vocabulary size: {vocab_size:,} words\")\n",
    "    \n",
    "    # Save the model\n",
    "    w2v_model.save('chatgpt_word2vec.model')\n",
    "    print(\"üíæ Model saved as 'chatgpt_word2vec.model'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error training Word2Vec: {e}\")\n",
    "    print(\"Skipping Word2Vec analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377de89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore word similarities\n",
    "if 'w2v_model' in locals():\n",
    "    print(\"üîç WORD SIMILARITY ANALYSIS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Test words related to our domain\n",
    "    test_words = ['chatgpt', 'openai', 'good', 'bad', 'great', 'terrible', 'amazing', 'awful']\n",
    "    available_words = [word for word in test_words if word in w2v_model.wv.key_to_index]\n",
    "    \n",
    "    print(f\"Available test words: {available_words}\")\n",
    "    \n",
    "    # Find similar words\n",
    "    for word in available_words[:5]:  # Limit to first 5 to save space\n",
    "        try:\n",
    "            similar_words = w2v_model.wv.most_similar(word, topn=5)\n",
    "            print(f\"\\nüî§ Words similar to '{word}':\")\n",
    "            for similar_word, similarity in similar_words:\n",
    "                print(f\"   {similar_word}: {similarity:.3f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   Error finding similarities for '{word}': {e}\")\n",
    "    \n",
    "    # Most common words in vocabulary\n",
    "    print(f\"\\nüìä Most common words in vocabulary:\")\n",
    "    word_counts = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word in w2v_model.wv.key_to_index:\n",
    "                word_counts[word] = word_counts.get(word, 0) + 1\n",
    "    \n",
    "    top_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    for i, (word, count) in enumerate(top_words, 1):\n",
    "        print(f\"   {i:2d}. {word}: {count:,} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef08984",
   "metadata": {},
   "source": [
    "## 10. Results Summary and Next Steps\n",
    "\n",
    "Let's summarize our findings and suggest next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f17792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"üìã ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"üìä Dataset Overview:\")\n",
    "print(f\"   ‚Ä¢ Total tweets analyzed: {len(df):,}\")\n",
    "print(f\"   ‚Ä¢ Sentiment classes: {df['labels'].nunique()} ({', '.join(df['labels'].unique())})\")\n",
    "print(f\"   ‚Ä¢ Average tweet length: {df['tweet_length'].mean():.1f} characters\")\n",
    "print(f\"   ‚Ä¢ Average word count: {df['word_count'].mean():.1f} words\")\n",
    "\n",
    "print(f\"\\nüéØ Model Performance:\")\n",
    "for model_name, accuracy in sorted_results:\n",
    "    print(f\"   ‚Ä¢ {model_name}: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nü•á Best Performing Model:\")\n",
    "print(f\"   ‚Ä¢ {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "\n",
    "if 'vocab_size' in locals():\n",
    "    print(f\"\\nüî§ Word2Vec Model:\")\n",
    "    print(f\"   ‚Ä¢ Vocabulary size: {vocab_size:,} words\")\n",
    "    print(f\"   ‚Ä¢ Vector dimensions: 100\")\n",
    "    print(f\"   ‚Ä¢ Model saved as: chatgpt_word2vec.model\")\n",
    "\n",
    "print(f\"\\nüíæ Generated Files:\")\n",
    "print(f\"   ‚Ä¢ chatgpt_tweets_analysis.png (data visualizations)\")\n",
    "print(f\"   ‚Ä¢ confusion_matrix.png (model evaluation)\")\n",
    "if 'w2v_model' in locals():\n",
    "    print(f\"   ‚Ä¢ chatgpt_word2vec.model (Word2Vec embeddings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040965a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggestions for next steps\n",
    "print(\"üöÄ NEXT STEPS & IMPROVEMENTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"1. üìà Model Improvements:\")\n",
    "print(\"   ‚Ä¢ Try deep learning models (LSTM, BERT)\")\n",
    "print(\"   ‚Ä¢ Experiment with different preprocessing techniques\")\n",
    "print(\"   ‚Ä¢ Use cross-validation for more robust evaluation\")\n",
    "print(\"   ‚Ä¢ Handle class imbalance if present\")\n",
    "\n",
    "print(\"\\n2. üîç Feature Engineering:\")\n",
    "print(\"   ‚Ä¢ Add sentiment-specific features (emoji, punctuation)\")\n",
    "print(\"   ‚Ä¢ Try different n-gram ranges\")\n",
    "print(\"   ‚Ä¢ Experiment with different vectorization parameters\")\n",
    "\n",
    "print(\"\\n3. üìä Advanced Analysis:\")\n",
    "print(\"   ‚Ä¢ Topic modeling with LDA\")\n",
    "print(\"   ‚Ä¢ Temporal analysis (if timestamps available)\")\n",
    "print(\"   ‚Ä¢ User behavior analysis\")\n",
    "print(\"   ‚Ä¢ Hashtag and mention analysis\")\n",
    "\n",
    "print(\"\\n4. üî§ Word2Vec Applications:\")\n",
    "print(\"   ‚Ä¢ Document clustering using embeddings\")\n",
    "print(\"   ‚Ä¢ Visualize word relationships with t-SNE\")\n",
    "print(\"   ‚Ä¢ Create custom sentiment lexicons\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis Complete!\")\n",
    "print(\"You now have a comprehensive understanding of your ChatGPT tweets dataset!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
